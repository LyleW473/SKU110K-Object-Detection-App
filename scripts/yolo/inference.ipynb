{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import set_path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_sliced_prediction, predict\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 8\n",
    "model_dir = f\"runs/detect/train{model_num}\"\n",
    "model_path = f\"{model_dir}/weights/best.pt\"\n",
    "print(model_path)\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "                                                    model_type=\"yolov8\",\n",
    "                                                    model_path=model_path,\n",
    "                                                    confidence_threshold=0.3,\n",
    "                                                    device=\"cpu\" if not torch.cuda.is_available() else \"cuda:0\"\n",
    "                                                    )\n",
    "with open(f\"{model_dir}/args.yaml\", \"r\") as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = \"test_33.jpg\"\n",
    "image_path = f\"data/filtered_images/{image_name}\"\n",
    "annotations_path = \"data/annotations/annotations_test.csv\"\n",
    "annotations = pd.read_csv(annotations_path)\n",
    "annotations.columns = [\"image_name\", \"x1\", \"y1\", \"x2\", \"y2\", \"class\", \"image_width\", \"image_height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_for_image = annotations[annotations[\"image_name\"] == image_name]\n",
    "annotations_for_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path:str) -> np.ndarray:\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def draw_image_with_annotations(image:np.ndarray, annotations:List[Tuple[int, int, int, int, str]]) -> np.ndarray:\n",
    "    for annotation in annotations:\n",
    "        x1, y1, x2, y2, class_name = annotation\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(image, class_name, (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = get_sliced_prediction(\n",
    "                                image=image_path,\n",
    "                                detection_model=detection_model,\n",
    "                                slice_width=config[\"imgsz\"], # Use the same slice width as the image size\n",
    "                                slice_height=config[\"imgsz\"], # Use the same slice width as the image size\n",
    "                                overlap_height_ratio=0.4,\n",
    "                                overlap_width_ratio=0.4\n",
    "                                )\n",
    "\n",
    "def extract_annotation(annotation_dict):\n",
    "    bbox = annotation_dict[\"bbox\"]\n",
    "    x1, y1, w, h = bbox\n",
    "    predicted_class = annotation_dict[\"category_name\"]\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    return [int(x1), int(y1), int(x2), int(y2), predicted_class]\n",
    "\n",
    "predictions = [extract_annotation(annotation) for annotation in result.to_coco_predictions()]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_annotations = annotations_for_image[[\"x1\", \"y1\", \"x2\", \"y2\", \"class\"]].values.tolist()\n",
    "gt_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_image_with_annotations(image=image, annotations=gt_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_image_with_annotations(image=image, annotations=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gt_annotations), len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
